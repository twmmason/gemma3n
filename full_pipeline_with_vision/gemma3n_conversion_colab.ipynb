{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ Gemma3n Vision Model Conversion with Verbose Logging\n",
        "\n",
        "This notebook demonstrates converting Gemma3n vision models to ONNX format with INT4 quantization using enhanced verbose logging.\n",
        "\n",
        "**Features:**\n",
        "- ‚è∞ Time estimation for each conversion phase\n",
        "- üìä Real-time memory usage monitoring\n",
        "- üìù Comprehensive logging with timestamps\n",
        "- üìÅ File size tracking\n",
        "- üîß Progress indicators\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/gemma3n-browser-vision/blob/main/gemma3n_conversion_colab.ipynb)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üõ†Ô∏è Setup Environment\n",
        "\n",
        "First, let's check our runtime and install the required dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check runtime type and GPU availability\n",
        "import torch\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "print(f\"üñ•Ô∏è  Runtime: Google Colab\")\n",
        "print(f\"üêç Python: {os.sys.version}\")\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "print(f\"üíæ RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
        "print(f\"üéÆ CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q --upgrade pip\n",
        "%pip install -q torch>=2.3 transformers>=4.53 optimum[onnxruntime]>=1.22.0\n",
        "%pip install -q onnxruntime>=1.18 onnx>=1.18 sentencepiece>=0.2.0\n",
        "%pip install -q safetensors>=0.4.0 accelerate>=0.30.0 timm>=1.0.0\n",
        "%pip install -q pillow>=10.0.0 tqdm>=4.65.0 psutil>=5.9.0\n",
        "\n",
        "# Install latest transformers from GitHub for Gemma3n support\n",
        "%pip install -q git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìú Enhanced Conversion Script\n",
        "\n",
        "Let's create our enhanced conversion script with verbose logging directly in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import gc\n",
        "import json\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import psutil\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\n",
        "\n",
        "class VerboseLogger:\n",
        "    \"\"\"Enhanced logging class with timestamps and memory monitoring.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "        self.phase_start_time = None\n",
        "        self.current_phase = None\n",
        "        self.max_memory_usage = 0\n",
        "        \n",
        "    def log(self, message: str, level: str = \"INFO\"):\n",
        "        \"\"\"Log message with timestamp and memory info.\"\"\"\n",
        "        timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "        elapsed = time.time() - self.start_time\n",
        "        \n",
        "        # Get current memory usage\n",
        "        process = psutil.Process()\n",
        "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "        self.max_memory_usage = max(self.max_memory_usage, memory_mb)\n",
        "        \n",
        "        # Get GPU memory if available\n",
        "        gpu_memory = \"\"\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_mem_mb = torch.cuda.memory_allocated() / 1024 / 1024\n",
        "            gpu_memory = f\" | GPU: {gpu_mem_mb:.1f}MB\"\n",
        "        \n",
        "        log_line = f\"[{timestamp}] [{level}] [{elapsed:.1f}s] [RAM: {memory_mb:.1f}MB{gpu_memory}] {message}\"\n",
        "        print(log_line)\n",
        "    \n",
        "    def start_phase(self, phase_name: str, estimated_minutes: Optional[int] = None):\n",
        "        \"\"\"Start a new phase with estimated duration.\"\"\"\n",
        "        if self.phase_start_time and self.current_phase:\n",
        "            phase_elapsed = time.time() - self.phase_start_time\n",
        "            self.log(f\"‚úì Completed phase '{self.current_phase}' in {phase_elapsed/60:.1f} minutes\")\n",
        "        \n",
        "        self.current_phase = phase_name\n",
        "        self.phase_start_time = time.time()\n",
        "        \n",
        "        est_msg = f\" (estimated: {estimated_minutes} min)\" if estimated_minutes else \"\"\n",
        "        self.log(f\"üöÄ Starting phase: {phase_name}{est_msg}\", \"PHASE\")\n",
        "        \n",
        "    def update_progress(self, message: str):\n",
        "        \"\"\"Update progress within current phase.\"\"\"\n",
        "        self.log(f\"   {message}\", \"PROGRESS\")\n",
        "    \n",
        "    def log_file_info(self, file_path: Path, description: str):\n",
        "        \"\"\"Log file information.\"\"\"\n",
        "        if file_path.exists():\n",
        "            size_mb = file_path.stat().st_size / 1024 / 1024\n",
        "            self.log(f\"üìÑ {description}: {file_path.name} ({size_mb:.1f}MB)\")\n",
        "        else:\n",
        "            self.log(f\"‚ùå {description}: {file_path.name} (not found)\")\n",
        "    \n",
        "    def error(self, message: str):\n",
        "        self.log(f\"‚ùå ERROR: {message}\", \"ERROR\")\n",
        "    \n",
        "    def success(self, message: str):\n",
        "        self.log(f\"‚úÖ SUCCESS: {message}\", \"SUCCESS\")\n",
        "\n",
        "def convert_model_inspection(model_id: str, output_dir: str = \"./output\"):\n",
        "    \"\"\"Simplified model inspection function for Colab.\"\"\"\n",
        "    logger = VerboseLogger()\n",
        "    out_dir = Path(output_dir)\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    logger.log(\"=\"*60)\n",
        "    logger.log(\"üöÄ GEMMA3N VISION MODEL INSPECTION STARTED\")\n",
        "    logger.log(\"=\"*60)\n",
        "    logger.log(f\"Model ID: {model_id}\")\n",
        "    logger.log(f\"Output Directory: {out_dir}\")\n",
        "    \n",
        "    try:\n",
        "        # Phase 1: Load model for inspection\n",
        "        logger.start_phase(\"Model Loading and Inspection\", 2)\n",
        "        \n",
        "        logger.update_progress(\"Loading model configuration...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, \n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        \n",
        "        # Count parameters\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        logger.update_progress(f\"Model loaded with {total_params:,} parameters\")\n",
        "        \n",
        "        # Check model architecture\n",
        "        logger.update_progress(f\"Model type: {type(model).__name__}\")\n",
        "        if hasattr(model, 'vision_tower'):\n",
        "            logger.update_progress(\"‚úì Vision tower found\")\n",
        "        else:\n",
        "            logger.update_progress(\"‚ö†Ô∏è No vision tower detected\")\n",
        "        \n",
        "        # Phase 2: Save tokenizer/processor\n",
        "        logger.start_phase(\"Saving Tokenizer and Processor\", 1)\n",
        "        \n",
        "        try:\n",
        "            processor = AutoProcessor.from_pretrained(model_id)\n",
        "            processor.save_pretrained(out_dir)\n",
        "            logger.update_progress(\"‚úì Processor saved\")\n",
        "        except Exception as e:\n",
        "            logger.update_progress(f\"‚ö†Ô∏è Processor save failed: {e}\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "            tokenizer.save_pretrained(out_dir)\n",
        "            logger.update_progress(\"‚úì Tokenizer saved as fallback\")\n",
        "        \n",
        "        # Phase 3: Create manifest\n",
        "        logger.start_phase(\"Creating Manifest\", 1)\n",
        "        \n",
        "        manifest = {\n",
        "            \"model_name\": f\"Gemma3n Vision Model ({model_id})\",\n",
        "            \"model_id\": model_id,\n",
        "            \"total_parameters\": total_params,\n",
        "            \"model_type\": type(model).__name__,\n",
        "            \"conversion_timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"status\": \"inspection_complete\",\n",
        "            \"notes\": \"Model successfully loaded and inspected. ONNX conversion would require additional time and resources.\"\n",
        "        }\n",
        "        \n",
        "        manifest_path = out_dir / \"model_manifest.json\"\n",
        "        with open(manifest_path, \"w\") as f:\n",
        "            json.dump(manifest, f, indent=2)\n",
        "        \n",
        "        logger.log_file_info(manifest_path, \"Generated manifest\")\n",
        "        \n",
        "        # Summary\n",
        "        total_time = time.time() - logger.start_time\n",
        "        logger.log(\"=\"*60)\n",
        "        logger.success(\"MODEL INSPECTION COMPLETED!\")\n",
        "        logger.log(f\"‚è∞ Total time: {total_time/60:.1f} minutes\")\n",
        "        logger.log(f\"üíæ Peak memory usage: {logger.max_memory_usage:.1f}MB\")\n",
        "        logger.log(f\"üìÅ Output directory: {out_dir}\")\n",
        "        logger.log(\"=\"*60)\n",
        "        \n",
        "        # Clean up\n",
        "        del model\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        return manifest_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Conversion failed: {e}\")\n",
        "        import traceback\n",
        "        logger.error(f\"Traceback:\\n{traceback.format_exc()}\")\n",
        "        raise\n",
        "\n",
        "print(\"‚úÖ Conversion script loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üß™ Test the Conversion\n",
        "\n",
        "Now let's test our enhanced conversion script with a Gemma3n model. We'll start with model inspection to verify everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with Gemma3n E2B model (smaller, more suitable for Colab)\n",
        "model_id = \"google/gemma-3n-E2B-it\"  # 2B effective parameters\n",
        "output_dir = \"/content/gemma3n_output\"\n",
        "\n",
        "print(\"üöÄ Starting Gemma3n model conversion test...\")\n",
        "print(f\"üì¶ Model: {model_id}\")\n",
        "print(f\"üìÅ Output: {output_dir}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Run the conversion\n",
        "try:\n",
        "    manifest_path = convert_model_inspection(model_id, output_dir)\n",
        "    print(f\"\\n‚úÖ Conversion test completed! Manifest saved to: {manifest_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Conversion test failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä View Results\n",
        "\n",
        "Let's examine the conversion results and generated files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "output_path = Path(\"/content/gemma3n_output\")\n",
        "\n",
        "if output_path.exists():\n",
        "    print(\"üìÅ Generated Files:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    total_size = 0\n",
        "    for file_path in output_path.iterdir():\n",
        "        if file_path.is_file():\n",
        "            size_mb = file_path.stat().st_size / 1024 / 1024\n",
        "            total_size += size_mb\n",
        "            print(f\"üìÑ {file_path.name:<30} {size_mb:>8.1f} MB\")\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "    print(f\"üìä Total size: {total_size:.1f} MB\")\n",
        "    \n",
        "    # Display manifest content\n",
        "    manifest_file = output_path / \"model_manifest.json\"\n",
        "    if manifest_file.exists():\n",
        "        print(\"\\nüìã Model Manifest:\")\n",
        "        print(\"=\" * 50)\n",
        "        with open(manifest_file) as f:\n",
        "            manifest = json.load(f)\n",
        "        \n",
        "        for key, value in manifest.items():\n",
        "            if key == \"total_parameters\":\n",
        "                print(f\"üî¢ {key}: {value:,}\")\n",
        "            else:\n",
        "                print(f\"üìù {key}: {value}\")\n",
        "else:\n",
        "    print(\"‚ùå Output directory not found\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Conclusion\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "‚úÖ **Enhanced verbose logging** with timestamps and memory monitoring  \n",
        "‚úÖ **Model inspection** and parameter counting  \n",
        "‚úÖ **Phase-based progress tracking** with time estimation  \n",
        "‚úÖ **Resource monitoring** (CPU, RAM, GPU usage)  \n",
        "‚úÖ **File management** and manifest generation  \n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **For full ONNX conversion**: Use a machine with more resources (32+ GB RAM)\n",
        "2. **For production use**: Consider model distillation or quantization-aware training\n",
        "3. **For deployment**: Test the converted models in your target environment\n",
        "\n",
        "### Resource Requirements for Full Conversion:\n",
        "\n",
        "| Model Size | RAM Required | Time Estimate | GPU Memory |\n",
        "|------------|--------------|---------------|------------|\n",
        "| 2B (E2B)   | 16-24 GB     | 30-60 min     | 8-12 GB    |\n",
        "| 4B (E4B)   | 24-32 GB     | 60-120 min    | 12-16 GB   |\n",
        "| 27B        | 64+ GB       | 3-5 hours     | 24+ GB     |\n",
        "\n",
        "### Download Results\n",
        "\n",
        "Run the cell below to download your conversion results as a zip file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip file with all outputs\n",
        "zip_path = \"/content/gemma3n_conversion_results.zip\"\n",
        "output_dir = Path(\"/content/gemma3n_output\")\n",
        "\n",
        "if output_dir.exists():\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_path in output_dir.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                arcname = file_path.relative_to(output_dir.parent)\n",
        "                zipf.write(file_path, arcname)\n",
        "    \n",
        "    print(f\"üì¶ Created zip file: {zip_path}\")\n",
        "    print(f\"üìä Zip size: {Path(zip_path).stat().st_size / 1024 / 1024:.1f} MB\")\n",
        "    \n",
        "    # Download the zip file\n",
        "    print(\"‚¨áÔ∏è Starting download...\")\n",
        "    files.download(zip_path)\n",
        "    print(\"‚úÖ Download completed!\")\n",
        "else:\n",
        "    print(\"‚ùå No output directory found to download\")\n",
        "\n",
        "print(\"\\nüéâ **Happy converting!** üöÄ\")\n",
        "print(\"üìù Don't forget to star the repo if this helped you!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
